{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install transformers datasets pdfplumber evaluate tqdm pymupdf\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NajE2Iix6zxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "77dzKvaF6xmX"
      },
      "outputs": [],
      "source": [
        "import pdfplumber\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from transformers import (\n",
        "    T5TokenizerFast,\n",
        "    T5ForConditionalGeneration,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "from datasets import load_dataset, Dataset\n",
        "import evaluate\n",
        "\n",
        "squad = load_dataset('squad')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def h_ans(example):\n",
        "\n",
        "    context = example['context']\n",
        "    answer = example['answers']['text'][0]\n",
        "    context_splits = context.split(answer)\n",
        "\n",
        "    highlighted_context = context_splits[0] + ' <h> ' + answer + ' <h> ' + context_splits[1]\n",
        "\n",
        "    return {'answer_highlighted_context': highlighted_context}\n",
        "\n",
        "\n",
        "h_ans_squad = squad.map(h_ans)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "54PB8lYg65or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def prepare_instruction_dataset(example):\n",
        "    \"\"\"\n",
        "    Prepare the instruction prompt for generating questions from highlighted answers in the context.\n",
        "    \"\"\"\n",
        "    answer_highlighted_context = example['answer_highlighted_context']\n",
        "\n",
        "    instruction_prompt = f\"\"\"Generate a question whose answer is highlighted by <h> from the context delimited by the triple backticks.\n",
        "    context:\n",
        "    ```\n",
        "    {answer_highlighted_context}\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    return {'instruction_prompt': instruction_prompt}\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "W9COKX5S7R1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_dataset(batch):\n",
        "    model_inputs = tokenizer(batch['instruction_prompt'], max_length=512, truncation=True, padding=True)\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(batch['question'], max_length=128, truncation=True, padding=True)\n",
        "\n",
        "    labels['input_ids'] = [\n",
        "        [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels['input_ids']\n",
        "    ]\n",
        "    model_inputs['labels'] = labels['input_ids']\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_squad = instruction_squad.map(tokenize_dataset, batched=True, remove_columns=squad['train'].column_names)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nWL33OIr7bGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='t5-small-squad-qg',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    evaluation_strategy='steps',\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=500,\n",
        "    save_steps=1000,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=True,\n",
        "    metric_for_best_model='eval_loss',\n",
        "    greater_is_better=False\n",
        ")\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2dWbsP5P7knk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_squad['train'],\n",
        "    eval_dataset=tokenized_squad['validation'],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "model.save_pretrained(\"t5-small-squad-qg\")\n",
        "tokenizer.save_pretrained(\"t5-small-squad-qg\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7uHFKEzG8VuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"t51-small-squad-qg\")\n",
        "tokenizer.save_pretrained(\"t51-small-squad-qg\")"
      ],
      "metadata": {
        "id": "l8dFpFhDN08g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extract text from a given PDF file.\n",
        "    \"\"\"\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text + '\\n'\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "8MqNPLVh8X9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_questions_from_text(text, model, tokenizer):\n",
        "\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    questions = []\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "\n",
        "        input_text = f\"Generate a question based on the following context: {paragraph}\"\n",
        "        input_ids = tokenizer.encode(input_text, return_tensors='pt', truncation=True)\n",
        "\n",
        "        outputs = model.generate(input_ids, max_length=128, num_beams=5, early_stopping=True)\n",
        "        question = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        questions.append(question)\n",
        "\n",
        "    return questions"
      ],
      "metadata": {
        "id": "oUrcfxHn9Qi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#trial func\n",
        "def generate_questions_from_pdfs(pdf_paths, model, tokenizer):\n",
        "\n",
        "    all_questions = {}\n",
        "\n",
        "    for pdf_path in pdf_paths:\n",
        "        text = extract_text_from_pdf(pdf_path)\n",
        "        questions = generate_questions_from_text(text, model, tokenizer)\n",
        "        all_questions[pdf_path] = questions\n",
        "\n",
        "    return all_questions"
      ],
      "metadata": {
        "id": "1rgyt91wNQcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained(\"/content/t51-small-squad-qg\")\n",
        "tokenizer = T5TokenizerFast.from_pretrained(\"/content/t51-small-squad-qg\")\n",
        "#our files over here:\n",
        "pdf_paths = [\n",
        "    '/content/MLsample.pdf',\n",
        "    '/content/NNsample.pdf',\n",
        "    '/content/cvsample.pdf',\n",
        "    '/content/nlpsample.pdf',\n",
        "    '/content/pythonsample.pdf'\n",
        "]\n"
      ],
      "metadata": {
        "id": "rMlNlCGUNT0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    return text\n",
        "\n",
        "def split_text_into_chunks(text, max_chunk_length=512):\n",
        "    chunks = []\n",
        "    sentences = text.split('. ')\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if len(current_chunk) + len(sentence) <= max_chunk_length:\n",
        "            current_chunk += sentence + \". \"\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence + \". \"\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "0NXbfJTVOF8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --force-reinstall pymupdf"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OJw5bAlFOaO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ckpt = \"/content/t51-small-squad-qg\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_ckpt)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_ckpt)\n",
        "def generate_questions_from_text_chunks(chunks, model, tokenizer):\n",
        "    questions = []\n",
        "    for chunk in tqdm(chunks):\n",
        "        input_text = f\"generate question: {chunk}\"\n",
        "        inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "        outputs = model.generate(inputs, max_length=150, num_beams=5, num_return_sequences=3)\n",
        "\n",
        "        for i in range(3):\n",
        "            question = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
        "            questions.append(question)\n",
        "\n",
        "    return questions"
      ],
      "metadata": {
        "id": "AyH5JhSZOdxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for pdf_path in pdf_paths:\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "    chunks = split_text_into_chunks(text)\n",
        "\n",
        "    questions = generate_questions_from_text_chunks(chunks, model, tokenizer)\n",
        "\n",
        "    print(f\"\\nQuestions from {pdf_path}:\")\n",
        "    for i, question in enumerate(questions, 1):\n",
        "        print(f\"{i}. {question}\")"
      ],
      "metadata": {
        "id": "QQ03SHhZO0mQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3LAvNqjLO46u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}